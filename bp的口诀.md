如果从caffe的角度来看问题的话
每一次的梯度是下面
bottom.data * top.diff

---

下面这个blog讲的不错

http://blog.csdn.net/zhongkejingwang/article/details/44514073

---
每个权重的梯度都等于与其相连的前一层节点的输出（即xi和θ(s1i)）乘以与其相连的后一层的反向传播的输出（即δ1j和δ2j）。


如果看不明白原理的话记住这句话即可！ 

---

这个如果推导的话，也会得到同样的结论。

你知道为啥吗，因为我们把深度模型的训练过程当作了一个数据优化的问题。

用的法则是链式求导。

刚才说的口诀就是链式求导的结果。

