为了避免文件名字太长，所以切断了。

问题是，神经网络为什么需要非线性激活函数，为什么relu 能够避免梯度消失。

---

## 第一个问题，为什么要引入激活函数
注意这里有定语，**非线性**

如果不用激励函数，其实就是相当于激活函数是$f(x)=x$


## 为什么使用relu
