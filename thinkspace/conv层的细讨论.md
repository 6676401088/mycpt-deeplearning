conv的用处
===

一个问题可以考察八成的功力：
CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN来出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性？
再补充一个问题，为什么很多做人脸的paper会最后加入一个local connected conv？

觉得能答对的同学可以私信我答案，我们可以直接进入面试流程哦~

作者：Naiyan Wang
链接：https://www.zhihu.com/question/41233373/answer/91113816
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

---

局部相关性

---
其实local conv是要结合对训练数据的alignment来用的，这也就是为什么google和vgg不用，而fb和yi sun用的原因。因为都对准了，所以人脸每个部位出现在图像中的位置是固定的，所以权值不用共享。这也算是人脸独有的一个先验了

---

因为convolution... 人脸最后用local connected的原因是因为不再convolution了因为人的眼睛不是满脸都有的。

求面试求蹭饭 :)

---

这个问题其实很不错，涉及到一个很本质的问题就是卷积为什么能够work - 这个可能的答案可以很多：

- 比如你可以从正则化的角度说这是一个很好地降低参数数量的方法
- 也可以从统计的角度说卷积很好地抓住了local correlation以及spatial invariance
- 也可以从编程的角度说卷积是一个很有效地提高计算效率的方法，表示你对工程很感兴趣
- 也可以从神经科学的角度说local connection是biologically inspired（但是你估计需要讨论一下为什么生物没有卷积这样共享参数的机制）

这几个角度可以很好地考察被试者在深度学习上的侧重点和经验，应该说是个甄别的好题目。

jiayangqing

---

全局特性受局部细节影响较小，然后cnn容忍扭曲能力逐层增强？

---

全局特性受局部细节影响较小，然后cnn容忍扭曲能力逐层增强？

接近了，但不准确

---

哈哈，多谢回答，之前看过google和stanford的nueral talk，用cnn提取的特征来做为rnn或者lstm的输入，然后结合rnn和word2vec来产生对图片的描述，但是关于直接运用cnn来对句子做卷积的了解不多，我想可能是因为卷积这样的操作可以提取局部特征，类比在语句中可以提取词汇之间的关系（猜测的。。。。。。），个人对cnn的理解其实来源于信号处理中的滤波器，卷积的操作可以看成傅立叶变换，而对图片进行不同的傅立叶变换可以得到得到不同的效果（去噪，锐化。。。。。。），cnn可以理解成去自动学习最有利于特定任务的滤波器？（脑洞），另外可以把滤波器看成一个个的小的模版，卷积的过程类似于模版匹配，匹配的越好，神经元激活值越大，训练过程也可以看成选取模版，一些思考，不足之处望批评指正。

可以利用不同尺度上的特征？

---

另外alphago和playing atari貌似都是将几幅连续的图片预处理后做为cnn的输入，做为当前的state，一个cnn做监督训练，target就是人类棋手在当前state的action，其实是训练机器模拟人类走棋，另外的一个cnn貌似没怎么看懂，只知道是用前一个cnn的参数初始化，具体如何左右互搏，尚在研究，亦希望有人能解答。

---

是有用cnn对sentence做的，作者的intuition就是对word甚至char这种单位组成进行抽象，得到句子的句法甚至语法方面的特征。。。其实和cnn在图像上的应用思想感觉差不多


---

CNN自动提取特征，避免了输入上的复杂处理，每一层只与上一层直接相连，且同一层共享值减少了节点数量，并使用sigmod使得映射保持唯一不变性。


---

正向计算，反向反馈调整取值


---

你是从优化和减少参数的角度来说的，当然这是对的，但是这并不能解释卷积这种操作的本质和含义。

---

好的特征: 1抽象能力、2、结构信息很重要，3、但结构也得容忍一定的变化，cnn的特性满足以上3点。


---

不就是一个条件概率么


---

CNN以上应用的核心在于捕捉输入信号的局部结构吧，图像是二维的，文本是一维的(句子)，当然也有考虑时域信息的三维卷积，不过核心都是为了捕捉局部结构吧。具体核心思路就是局部连接，权值共享，然后通过多层的网络结构来捕捉不同分辨率下的特征。个人理解，求指正。至于人脸那个不是很了解。


---

CNN的特点在于其局部相关性，只要带有该特性的问题都可以用CNN来尝试解决。随着现在网络深度加深，高层的mapsize减小，高层特征具有更高的抽象程度与视野区域，local conv层的作用已经不像以前那么重要了。
求面试求蹭饭+1

---


那看来对dl领域的人才应该进一步细分为idea contributor和code contributor。但是即便是idea contributor，这些问题的分类能力似乎也不够好：因为idea是基于直觉而不是推理的，甚至是靠直觉产生了idea然后再去找解释的。不少人确实很理解dl的工作原理，但是并没有idea，还是要靠海试现有方法并补漏洞，最终还是变成了code contributor。
我觉得如果我是team leader的话，我会尽量找代码功底强的，至于到底对cnn有多少理解不关键。因为情况多是idea多，但是实现少！随你了解caf还是tf还是mx还是t7还是d4j，只要能跑起来就好了。不理解？读paper！

---

你说的好有道理！看来这是要求面试官和来面试的很会聊，哈哈
生物其实是有共享参数的，但是和更像是replication而不是convolution，因而作用不明。dropout，pooling倒是有很明确的生物学对应的。

---

我觉得深度学习问题适合做加分项，代码能力和机器学习基础比较能判断一个人的真正水平。

---

你说的是招纯工程师的思路，我们想要的不仅仅是工程师啊 :)


---

局部特征+组合抽象


---
首先神经网络理论上可以无限接近任意loss function。其次特别就cnn而言，它有效的保留了输入的空间特征。以图片为例，前几层的卷积层感受野小，用于检测图片底层局部特征，而高层卷积感受野逐步增大，抽取高层抽象特征。nlp我不是特别了解，但我觉得原理是一样的，首先是词和词直接的关系，然后是context之间的关系，逐步由局部扩展到全局，由底层特征扩展到高层抽象特征。刚入坑的小白。求大神指点

---

1 这些问题的共性在于其都在不同层次上拥有各自的特征。
2 CNN的优点在于自动提取不同层次的特征。卷积层相当于在特征上增维，降采样层相当于在尺度上降唯。卷积层的connection table暗示了更高尺度的特征等于上一个小尺度的特征的线性组合。并且要尝试遍历这种组合。
3 卷积的特性使得并行训练成为可能。现在大样本下基本是使用CUDA的硬件加速功能进行大样本训练的。
