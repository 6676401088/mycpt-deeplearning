著作权归作者所有。
商业转载请联系作者获得授权，非商业转载请注明出处。
作者：张馨宇
链接：https://www.zhihu.com/question/38499534/answer/76826468
来源：知乎

deep learning有个问题是越深越难训，
微软这个工作的直观想法应该就是说既然太深了难训，那我就把前面的层，夸几层直接接到后面去，会不会既保证了深度，又让前面的好训一些呢？

其实这个并不是特别新的想法，这个训得这么好，应该也是经过了很辛苦和仔细的调参、初始化等工作。

换一个角度看，这也是一种把高阶特征和低阶特征再做融合，从而得到更好的效果的思路。

---

concat 算子就可以做这件事情。

---

qhj的话

学习深度学习，有很多很多路可以。

1. 算子(conv,pooling) 不光要学习这些算子的用法，还要学习一些这个算子的来源，以及为何是如此的样
2. 模型（alexnet，Inception）
3. 认知神经科学
4. 平台
